# fastText

페이스북에서 개발한 **텍스트 임베딩 기법** 중 하나로, 전반적으로 word2vec과 유사하나 가장 큰 차이점은 **word2vec**은 **한 단어를 더 이상 쪼개질 수 없는 최소 단위**라 생각한다면, **fastText**는 **각 단어를 character 단위 n-gram으로 쪼갠다**는 것이다. 즉 내부 단어를 고려하며 학습하여 word2Vec의 한계를 보완한다.

<br/>

### 1. fastText의 원리

#### 1-a 내부 단어(subword)의 학습

n의 값에 따라 단어가 얼마나 분리되는지 결정된다.

예를 들어 n = 3 일 때, happy 라는 단어는 `hap` `app` `ppy` + 시작과 끝을 의미하는 `<`, `>` 를 도입하여 `<ha`, `py>` 총 다섯가지의 내부 단어를 벡터화 시킨다.

```python
# n = 3
# 단어 = happy
<ha, hap, app, ppy, py>
```

여기에 마지막으로 원래의 단어에 `<`, `>` 를 추가해준 `<happy>` 벡터까지 추가를 해서 총 여섯개의 단어를 벡터화 시키게 된다.

```python
# n = 3
# 단어 = happy
<ha, hap, app, ppy, py>, <happy>
```

<br/>

실제 사용할 때는 n의 최소값과 최대값을 설정할 수 있는데 기본값으로는 각각 3, 6 으로 설정되어있다. 따라서 n이 3~6인 모든 경우에 대해서 벡터화를 진행시키고(Word2vec을 수행한다는 의미) 단어 happy의 벡터값은 모든 벡터 값의 총 합으로 구성된다.

```python
# n = 3 ~ 6인 모든 경우에 대해
happy = <ha + hap + app + ppy + py> ..... + <happy>
```

<br/>

#### 1-b 한국어 학습

영어에서 각 알파벳으로 분리하듯이 한국어를 자음, 모음으로 분리하여 이용을 하게 되는데 문장 전체를 그대로 하기보다는 형태소 분리를 이용하여 토큰화 한 후 자소 분리를 진행한다.

##### 음절 단위

음절 단위의 임베딩의 경우에 n=3일때 `자연어처리`라는 단어에 대해 n-gram을 만들어보면 다음과 같다.

```
<자연, 자연어, 연어처, 어처리, 처리>
```

##### 자모 단위

하지만 이제 더 나아가 자모 단위(초성, 중성, 종성 단위)로 임베딩하는 시도 또한 있었따. 음절 단위가 아니라, 자모 단위로 가게 되면 오타나 노이즈 측면에서 더 강한 임베딩을 기대해볼 수 있다. `자연어처리`라는 단어에 대해서 초성, 중성, 종성을 분리하고, 만약, 종성이 존재하지 않는다면 `_`라는 토큰을 사용한다고 가정한다면 `자연어처리`라는 단어는 아래와 같이 분리가 가능하다.

```
ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _
```

그리고 분리된 결과에 대해서 n=3일 때, n-gram을 적용하여, 임베딩을 한다면 다음과 같다.

```python
# n = 3 
< ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... 중략>
```

<br/>

### 2. fastText만의 장점

#### 2-a 모르는 단어 (Out Of Vocabulary, OOV)에 대한 대응

FastText의 인공 신경망을 학습한 후에는 데이터 셋의 모든 단어의 각 n-gram에 대해 워드 임베딩이 된다.

이에 대한 장점으로는, **내부 단어**들을 통해 **모르는 단어(OOV)와의 유사도도 계산**할 수 있다.

<br/>

예를 들어, `birthday`라는 단어를 학습하지 않은 상태일 때:

- FastText: `birth`와 `day`라는 내부 단어에 대한 학습이 되었을 때 `birthday`에 대한 벡터를 얻을 수 있다.
- Word2Vec, GloVe: 학습 데이터에 존재하지 않는 단어에 대해서는 벡터를 얻을 수 없다.

<br/>

#### 2-b 출현 빈도수가 적은 단어에 대한 대응

Word2Vec은 등장 빈도수가 적은 단어에 대해서는 임베딩의 정확도가 떨어진다는 단점이 있다. 예를 들어 `happy`와` happpy`는 다른 단어로 인식하게 된다.

FastText는 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, 유사도를 계산해서 임베딩 성능을 계산한다.



```python
# happy, n = 3
<ha, hap, app, ppy, py>, <happy>

# happpy, n = 3
<ha, hap, app, ppp, ppy, py>, <happpy>

```



따라서 fastText는 **노이즈가 많은 코퍼스에서 강점**을 가지고 있다. 오타 같은 경우에는 비교적 빈도수가 적기 때문에 희귀단어가 되는데, Word2Vec에서는 오타가 섞인 단어는 다른 단어로 임베딩이 되는 반면에 FastText는 일정 수준의 성능을 보인다.



이에 더해 같은 어원을 가지고 있는 단어에 대한 유사도를 계산할 수 있다.

`dislike`: `dis` + `like`

`disappear`: `dis` + `appear`

`dis` : 부정어





<br/>

### 3. 실습을 통해 알아보는 Word2Vec vs FastText

[word2vec_fastText.ipynb](https://github.com/sophryu99/machine-learning/blob/main/NLP/word2vec_fastText.ipynb)



<br/>

[참고링크1](https://wikidocs.net/22883)

[참고링크2](https://simonezz.tistory.com/54)

[참고링크3](https://towardsdatascience.com/fasttext-ea9009dba0e8)

