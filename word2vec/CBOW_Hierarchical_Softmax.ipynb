{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CBOW Hierarchical Softmax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2eeRA3nPlFZ"
      },
      "source": [
        "참고 : https://github.com/weberrr/pytorch_word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh_YgWJK8pPx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLOvIRR4Cecn",
        "outputId": "1b36ab13-4460-44cc-df91-4e306cf86c34"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)    "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSPSVuG_CLJ"
      },
      "source": [
        "### Huffman tree 쌓기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgA6GQJY_CZj"
      },
      "source": [
        "class HuffmanNode:\n",
        "    def __init__(self, word_id, frequency):\n",
        "        self.word_id = word_id  \n",
        "        self.frequency = frequency \n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "        self.father = None\n",
        "        self.Huffman_code = []  \n",
        "        self.path = []  \n",
        "\n",
        "\n",
        "class HuffmanTree:\n",
        "    def __init__(self, wordid_frequency_dict):\n",
        "        self.word_count = len(wordid_frequency_dict)  \n",
        "        self.wordid_code = dict()\n",
        "        self.wordid_path = dict()\n",
        "        self.root = None\n",
        "        unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in\n",
        "                             wordid_frequency_dict.items()] \n",
        "        self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in\n",
        "                        wordid_frequency_dict.items()]  \n",
        "        # huffman tree\n",
        "        self.build_tree(unmerge_node_list)\n",
        "        # huffman code\n",
        "        self.generate_huffman_code_and_path()\n",
        "\n",
        "    def merge_node(self, node1, node2):\n",
        "        sum_frequency = node1.frequency + node2.frequency\n",
        "        mid_node_id = len(self.huffman) \n",
        "        father_node = HuffmanNode(mid_node_id, sum_frequency)\n",
        "        if node1.frequency >= node2.frequency: # 빈도수가 높으면 왼쪽\n",
        "            father_node.left_child = node1\n",
        "            father_node.right_child = node2\n",
        "        else:\n",
        "            father_node.left_child = node2\n",
        "            father_node.right_child = node1\n",
        "        self.huffman.append(father_node)\n",
        "        return father_node\n",
        "\n",
        "    def build_tree(self, node_list):\n",
        "        while len(node_list) > 1:\n",
        "            i1 = 0  \n",
        "            i2 = 1  \n",
        "            if node_list[i2].frequency < node_list[i1].frequency:\n",
        "                [i1, i2] = [i2, i1]\n",
        "            for i in range(2, len(node_list)):\n",
        "                if node_list[i].frequency < node_list[i2].frequency:\n",
        "                    i2 = i\n",
        "                    if node_list[i2].frequency < node_list[i1].frequency:\n",
        "                        [i1, i2] = [i2, i1]\n",
        "            father_node = self.merge_node(node_list[i1], node_list[i2])  \n",
        "            if i1 < i2:\n",
        "                node_list.pop(i2)\n",
        "                node_list.pop(i1)\n",
        "            elif i1 > i2:\n",
        "                node_list.pop(i1)\n",
        "                node_list.pop(i2)\n",
        "            else:\n",
        "                raise RuntimeError('i1 should not be equal to i2')\n",
        "            node_list.insert(0, father_node)  \n",
        "        self.root = node_list[0]\n",
        "\n",
        "    def generate_huffman_code_and_path(self):\n",
        "        stack = [self.root]\n",
        "        while len(stack) > 0:\n",
        "            node = stack.pop()\n",
        "            \n",
        "            while node.left_child or node.right_child:\n",
        "                code = node.Huffman_code\n",
        "                path = node.path\n",
        "                node.left_child.Huffman_code = code + [1] # 좌로 가면 1\n",
        "                node.right_child.Huffman_code = code + [0] # 우로 가면 0\n",
        "                node.left_child.path = path + [node.word_id]\n",
        "                node.right_child.path = path + [node.word_id]\n",
        "                \n",
        "                stack.append(node.right_child)\n",
        "                node = node.left_child\n",
        "            word_id = node.word_id\n",
        "            word_code = node.Huffman_code\n",
        "            word_path = node.path\n",
        "            self.huffman[word_id].Huffman_code = word_code\n",
        "            self.huffman[word_id].path = word_path\n",
        "            \n",
        "            self.wordid_code[word_id] = word_code\n",
        "            self.wordid_path[word_id] = word_path\n",
        "\n",
        "    \n",
        "    def get_all_pos_and_neg_path(self):\n",
        "        positive = []  # 왼쪽\n",
        "        negative = []  # 오르쪽\n",
        "        for word_id in range(self.word_count):\n",
        "            pos_id = []  \n",
        "            neg_id = []  \n",
        "            for i, code in enumerate(self.huffman[word_id].Huffman_code):\n",
        "                if code == 1:\n",
        "                    pos_id.append(self.huffman[word_id].path[i])\n",
        "                else:\n",
        "                    neg_id.append(self.huffman[word_id].path[i])\n",
        "            positive.append(pos_id)\n",
        "            negative.append(neg_id)\n",
        "        return positive, negative"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuQ9ZPa3_DPD"
      },
      "source": [
        "### input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa2SBkta_Dcv"
      },
      "source": [
        "class InputData:\n",
        "    def __init__(self, input_file_name, min_count):\n",
        "        self.input_file_name = input_file_name\n",
        "        self.input_file = open(self.input_file_name)  \n",
        "        self.min_count = min_count  \n",
        "        self.wordId_frequency_dict = {}\n",
        "        self.word_count = 0  \n",
        "        self.word_count_sum = 0\n",
        "        self.sentence_count = 0  \n",
        "        self.id2word_dict = {}\n",
        "        self.word2id_dict = {}\n",
        "        self._init_dict()  \n",
        "        self.huffman_tree = HuffmanTree(self.wordId_frequency_dict) \n",
        "        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n",
        "        self.word_pairs_queue = deque()\n",
        "        \n",
        "        print('Word Count is:', self.word_count)\n",
        "        print('Word Count Sum is', self.word_count_sum)\n",
        "        print('Sentence Count is:', self.sentence_count)\n",
        "        print('Tree Node is:', len(self.huffman_tree.huffman))\n",
        "\n",
        "    def _init_dict(self):\n",
        "        word_freq = {}\n",
        "        \n",
        "        for line in self.input_file:\n",
        "            line = line.strip().split(' ')  \n",
        "            self.word_count_sum += len(line)\n",
        "            self.sentence_count += 1\n",
        "            for word in line:\n",
        "                try:\n",
        "                    word_freq[word] += 1\n",
        "                except:\n",
        "                    word_freq[word] = 1\n",
        "        word_id = 0\n",
        "        \n",
        "        for per_word, per_count in word_freq.items():\n",
        "            if per_count < self.min_count:  \n",
        "                self.word_count_sum -= per_count\n",
        "                continue\n",
        "            self.id2word_dict[word_id] = per_word\n",
        "            self.word2id_dict[per_word] = word_id\n",
        "            self.wordId_frequency_dict[word_id] = per_count\n",
        "            word_id += 1\n",
        "        self.word_count = len(self.word2id_dict)\n",
        "\n",
        "    \n",
        "    def get_batch_pairs(self, batch_size, window_size):\n",
        "        while len(self.word_pairs_queue) < batch_size:\n",
        "            for _ in range(10000):  \n",
        "                self.input_file = open(self.input_file_name, encoding=\"utf-8\")\n",
        "                sentence = self.input_file.readline()\n",
        "                if sentence is None or sentence == '':\n",
        "                    continue\n",
        "                wordId_list = []  \n",
        "                for word in sentence.strip().split(' '):\n",
        "                    try:\n",
        "                        word_id = self.word2id_dict[word]\n",
        "                        wordId_list.append(word_id)\n",
        "                    except:\n",
        "                        continue\n",
        "                \n",
        "                for i, wordId_w in enumerate(wordId_list):\n",
        "                    context_ids = []\n",
        "                    for j, wordId_u in enumerate(wordId_list[max(i - window_size, 0):i + window_size + 1]):\n",
        "                        assert wordId_w < self.word_count\n",
        "                        assert wordId_u < self.word_count\n",
        "                        if i == j:  \n",
        "                            continue\n",
        "                        elif max(0, i - window_size + 1) <= j <= min(len(wordId_list), i + window_size - 1):\n",
        "                            context_ids.append(wordId_u)\n",
        "                    if len(context_ids) == 0:\n",
        "                        continue\n",
        "                    self.word_pairs_queue.append((context_ids, wordId_w))\n",
        "        result_pairs = []  \n",
        "        for _ in range(batch_size):\n",
        "            result_pairs.append(self.word_pairs_queue.popleft())\n",
        "        return result_pairs\n",
        "\n",
        "    def get_pairs(self, pos_pairs):\n",
        "        neg_word_pair = []\n",
        "        pos_word_pair = []\n",
        "        for pair in pos_pairs:\n",
        "            pos_word_pair += zip([pair[0]] * len(self.huffman_pos_path[pair[1]]), self.huffman_pos_path[pair[1]])\n",
        "            neg_word_pair += zip([pair[0]] * len(self.huffman_neg_path[pair[1]]), self.huffman_neg_path[pair[1]])\n",
        "        return pos_word_pair, neg_word_pair\n",
        "\n",
        "    \n",
        "    def evaluate_pairs_count(self, window_size):\n",
        "        return self.word_count_sum * (2 * window_size - 1)  - (self.sentence_count - 1) * (1 + window_size) * window_size"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Jf66km_Dpo"
      },
      "source": [
        "### CBOW model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qQq3wM7_D2I"
      },
      "source": [
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(2*self.emb_size-1, self.emb_dimension, sparse=True)   #(단어집합크기, 임베딩할 벡터의 차원, sparse)\n",
        "        self.w_embeddings = nn.Embedding(2*self.emb_size-1, self.emb_dimension, sparse=True)\n",
        "        self._init_embedding()\n",
        "    \n",
        "    def _init_embedding(self):\n",
        "        int_range = 0.5 / self.emb_dimension\n",
        "        self.u_embeddings.weight.data.uniform_(-int_range, int_range) # 초기 가중치\n",
        "        self.w_embeddings.weight.data.uniform_(-int_range, int_range)\n",
        "        \n",
        "    def compute_context_matrix(self, u):\n",
        "        pos_u_emb = []\n",
        "        for per_Xw in u:\n",
        "            per_u_emb = self.u_embeddings(torch.LongTensor(per_Xw))   #64비트의 부호 있는 정수 -> LongTensor\n",
        "            # Xw = Vwi\n",
        "            Vwi = per_u_emb.data.numpy()\n",
        "            Vwi_numpy = np.sum(Vwi, axis=0)\n",
        "            Vwi_list = Vwi_numpy.tolist()\n",
        "            pos_u_emb.append(Vwi_list)\n",
        "        pos_u_emb = torch.FloatTensor(pos_u_emb)\n",
        "        return pos_u_emb\n",
        "    \n",
        "    def forward(self, pos_u, pos_w, neg_u, neg_w):\n",
        "        pos_u_emb = self.compute_context_matrix(pos_u)\n",
        "        pos_w_emb = self.w_embeddings(torch.LongTensor(pos_w))\n",
        "        neg_u_emb = self.compute_context_matrix(neg_u)\n",
        "        neg_w_emb = self.w_embeddings(torch.LongTensor(neg_w))\n",
        "        \n",
        "        score_1 = torch.mul(pos_u_emb, pos_w_emb).squeeze() # 자녀가 왼쪽 노드\n",
        "        score_2 = torch.sum(score_1, dim=1)\n",
        "        score_3 = F.logsigmoid(-1 * score_2)\n",
        "        neg_score_1 = torch.mul(neg_u_emb, neg_w_emb).squeeze() # 자녀가 오른쪽 노드\n",
        "        neg_score_2 = torch.sum(neg_score_1, dim=1)\n",
        "        neg_score_3 = F.logsigmoid(neg_score_2)\n",
        "        \n",
        "        loss = torch.sum(score_3) + torch.sum(neg_score_3) # P(w | wi)에 log를 씌웠으므로 합으로 표현해도 무방\n",
        "        return -1 * loss # 최소화 문제로 변환\n",
        "    \n",
        "    def save_embedding(self, id2word_dict, file_name):\n",
        "        embeddings = self.u_embeddings.weight.data.numpy()\n",
        "        file_output = open(file_name, 'w')\n",
        "        file_output.write('%d %d\\n' % (self.emb_size, self.emb_dimension))\n",
        "        for id, word in id2word_dict.items():\n",
        "            e = embeddings[id]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            file_output.write('%s %s\\n' % (word, e))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anOnaBqL_EB2"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8NWqewi_la8"
      },
      "source": [
        "WINDOW_SIZE = 4  \n",
        "BATCH_SIZE = 10  \n",
        "MIN_COUNT = 1  \n",
        "EMB_DIMENSION = 100  \n",
        "LR = 0.02  \n",
        "NEG_COUNT = 4  \n",
        "\n",
        "\n",
        "class Word2Vec:\n",
        "    def __init__(self, input_file_name, output_file_name):\n",
        "        self.output_file_name = output_file_name\n",
        "        self.data = InputData(input_file_name, MIN_COUNT)\n",
        "        self.model = CBOWModel(self.data.word_count, EMB_DIMENSION)\n",
        "        self.lr = LR\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "    def train(self):\n",
        "        print(\"CBOW Training......\")\n",
        "        pairs_count = self.data.evaluate_pairs_count(WINDOW_SIZE)\n",
        "        print(\"pairs_count\", pairs_count)\n",
        "        batch_count = pairs_count / BATCH_SIZE\n",
        "        print(\"batch_count\", batch_count)\n",
        "        process_bar = tqdm(range(int(batch_count)))\n",
        "        for i in process_bar:\n",
        "            pos_pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE)\n",
        "            pos_pairs, neg_pairs = self.data.get_pairs(pos_pairs)\n",
        "\n",
        "            pos_u = [pair[0] for pair in pos_pairs]\n",
        "            pos_v = [int(pair[1]) for pair in pos_pairs]\n",
        "            neg_u = [pair[0] for pair in neg_pairs]\n",
        "            neg_v = [int(pair[1]) for pair in neg_pairs]\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.model.forward(pos_u,pos_v,neg_u,neg_v)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if i * BATCH_SIZE % 100000 == 0:\n",
        "                self.lr = self.lr * (1.0 - 1.0 * i / batch_count)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = self.lr\n",
        "\n",
        "        self.model.save_embedding(self.data.id2word_dict, self.output_file_name)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA-tbrp6_yGr",
        "outputId": "3447d1f0-6e3a-477f-bcf6-6cc4d48c255b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDbwiQUkA0Cj",
        "outputId": "bb16417f-1619-4979-8cd7-0dea1016882f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    word2vec = Word2Vec(input_file_name='/content/drive/MyDrive/DSL/embedding/martinluther.txt', output_file_name=\"word_embedding.txt\")\n",
        "    word2vec.train()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1399 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word Count is: 695\n",
            "Word Count Sum is 2150\n",
            "Sentence Count is: 54\n",
            "Tree Node is: 1389\n",
            "CBOW Training......\n",
            "pairs_count 13990\n",
            "batch_count 1399.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1399/1399 [00:12<00:00, 111.28it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5gMqPZQtA1xk",
        "outputId": "39b392cc-b462-40c1-f93c-771484b52bdc"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"word_embedding.txt\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b67a3675-5ffb-4a66-8bf5-6843313ca82f\", \"word_embedding.txt\", 934034)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qwxXXoZW_4S"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}